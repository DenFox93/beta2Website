<!doctype html>
<html>
<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <title>wget</title>
  <meta name="generator" content="CherryTree">
  <link rel="stylesheet" href="res/styles4.css" type="text/css" />
</head>
<body>
<div class='page'><h1 class='title'>wget</h1><br/><br />Wget can function as a simple spider, using the --recursive option. When using the -r option, wget extracts links from pages and downloads each linked page, recursively repeating this process until all resources have been downloaded or the maximum recursion depth set by the user has been reached.<br /><br />This tells wget to recursively spider the website to a link depth of 3. If we not specify with -P the folder of destination, wget will create automatically a folder with the name of the site<br /><div class="codebox"><pre>root@kali<span style="color:#ff9d00;font-weight:700">:/</span># <span style="color:#ff9d00;font-weight:700">wget</span> -r http<span style="color:#ff9d00;font-weight:700">://</span>[site] -l 3 -P [folderDestination]<br />root@kali<span style="color:#ff9d00;font-weight:700">:/</span># <span style="color:#ff9d00;font-weight:700">cd</span> [folderDestination]<br />root@kali<span style="color:#ff9d00;font-weight:700">:/</span># <span style="color:#ff9d00;font-weight:700">ls</span></pre></div><br />    <a href=""><img src="images/127-1.png" alt="images/127-1.png" /></a></div>
</body>
</html>
